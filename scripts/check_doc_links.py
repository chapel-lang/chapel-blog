import argparse
import json
import functools
import urllib.parse
from collections import defaultdict
from requests import get
from bs4 import BeautifulSoup

def main():
    parser = argparse.ArgumentParser(description="Check links generated by insert_links.py")
    parser.add_argument("cachefile", type=str, help="Path to the cache file containing links")
    args = parser.parse_args()
    code = 0

    with open(args.cachefile, 'r') as file:
        cache_data = json.load(file)

    # the code below assumes correct cache format

    url_fragments = defaultdict(set)

    for file, data in cache_data.items():
        for (rng, url) in data['references']:
            parsed = urllib.parse.urlparse(url)
            canon_url = parsed.scheme + '://' + parsed.netloc + parsed.path
            url_fragments[canon_url].add(parsed.fragment)

    for url, fragments in url_fragments.items():
        page = get(url)
        soup = BeautifulSoup(page.text, 'html.parser')

        for fragment in fragments:
            if fragment == '': continue
            if not soup.find(id=fragment):
                print(f"Fragment '{fragment}' not found in {url}")
                code = 1

    exit(code)

if __name__ == "__main__":
    main()
